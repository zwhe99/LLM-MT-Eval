# LLM-MT-Eval

This repo evaluates

* DeepL
* Google Trans
* WMT22 Best
* text-davinci-003
* gpt-3.5-turbo-0301
* gpt-4-0314

in automatic metrics:

* COMET
* BLEURT
* BLEU
* chrF
* chrF++

 on WMT22 general translation tasks:

* English$\Leftrightarrow$German
* English$\Leftrightarrow$Czech
* English$\Leftrightarrow$Russian
* English$\Leftrightarrow$Chinese
* German$\Leftrightarrow$French
* English$\Leftrightarrow$Japanese
* Ukrainian$\Leftrightarrow$English
* Ukrainian$\Leftrightarrow$Czech
* English$\Rightarrow$Croatian.

